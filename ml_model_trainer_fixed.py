"""
FIXED Machine Learning Model for Healthcare Fraud Detection
NO DATA LEAKAGE - Uses only legitimate features
Realistic performance expected: 85-95% accuracy
"""

import pandas as pd
import numpy as np
import pickle
import os
from datetime import datetime
from typing import Dict, Tuple, List

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    accuracy_score, precision_score, recall_score, f1_score
)

try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸  XGBoost not available. Install with: pip install xgboost")

try:
    from lightgbm import LGBMClassifier
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸  LightGBM not available. Install with: pip install lightgbm")

import warnings
warnings.filterwarnings('ignore')


class FraudDetectionMLModel:
    """ML-based fraud detection - NO DATA LEAKAGE VERSION"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_names = []
        self.feature_importance = None
        self.model_dir = 'models'
        os.makedirs(self.model_dir, exist_ok=True)
        
    def engineer_features_no_leakage(self, claims_df: pd.DataFrame, providers_df: pd.DataFrame, 
                                     patients_df: pd.DataFrame) -> pd.DataFrame:
        """Create features WITHOUT using any fraud indicators"""
        
        print("ğŸ”§ Engineering features (NO DATA LEAKAGE)...")
        
        # Merge with providers and patients
        claims = claims_df.copy()
        
        # IMPORTANT: DO NOT USE risk_score or fraud_history_count!
        # Only use legitimate provider attributes
        provider_features = providers_df[['provider_id', 'specialty', 'years_experience', 'license_number']].copy()
        claims = claims.merge(provider_features, on='provider_id', how='left', suffixes=('', '_provider'))
        
        patient_features = patients_df[['patient_id', 'age', 'gender', 'state', 'member_since']].copy()
        claims = claims.merge(patient_features, on='patient_id', how='left', suffixes=('', '_patient'))
        
        features = pd.DataFrame(index=claims.index)
        
        # 1. CLAIM AMOUNT FEATURES (NO LEAKAGE - just statistical)
        features['claim_amount'] = claims['claim_amount']
        features['log_amount'] = np.log1p(claims['claim_amount'])
        features['amount_squared'] = claims['claim_amount'] ** 2
        features['amount_sqrt'] = np.sqrt(claims['claim_amount'])
        
        # Amount percentile within dataset
        features['amount_percentile'] = claims['claim_amount'].rank(pct=True)
        
        # 2. SPECIALTY-BASED FEATURES (NO LEAKAGE)
        # Average amount for this specialty (calculated from ALL claims)
        specialty_stats = claims.groupby('specialty_provider')['claim_amount'].agg(['mean', 'std', 'median'])
        specialty_stats.columns = ['spec_mean', 'spec_std', 'spec_median']
        claims = claims.merge(specialty_stats, left_on='specialty_provider', right_index=True, how='left')
        
        features['specialty_avg_amount'] = claims['spec_mean']
        features['specialty_std_amount'] = claims['spec_std']
        features['amount_vs_specialty_median'] = claims['claim_amount'] / (claims['spec_median'] + 1)
        
        # Z-score relative to specialty (normalized deviation)
        features['specialty_z_score'] = (claims['claim_amount'] - claims['spec_mean']) / (claims['spec_std'] + 1e-6)
        
        # 3. PROCEDURE-BASED FEATURES (NO LEAKAGE)
        # Average for this specific procedure code
        proc_stats = claims.groupby('cpt_code')['claim_amount'].agg(['mean', 'std', 'count'])
        proc_stats.columns = ['proc_mean', 'proc_std', 'proc_count']
        claims = claims.merge(proc_stats, left_on='cpt_code', right_index=True, how='left')
        
        features['procedure_avg_amount'] = claims['proc_mean']
        features['procedure_std_amount'] = claims['proc_std']
        features['procedure_frequency'] = claims['proc_count']
        features['procedure_z_score'] = (claims['claim_amount'] - claims['proc_mean']) / (claims['proc_std'] + 1e-6)
        
        # 4. PROVIDER FEATURES (ONLY LEGITIMATE ATTRIBUTES - NO FRAUD HISTORY!)
        features['provider_years_experience'] = claims['years_experience']
        
        # Provider's claim count (volume indicator)
        provider_counts = claims.groupby('provider_id').size()
        features['provider_claim_volume'] = claims['provider_id'].map(provider_counts)
        
        # Provider's average claim amount (could indicate billing patterns)
        provider_avg = claims.groupby('provider_id')['claim_amount'].agg(['mean', 'std'])
        provider_avg.columns = ['prov_mean', 'prov_std']
        claims = claims.merge(provider_avg, left_on='provider_id', right_index=True, how='left')
        features['provider_avg_claim'] = claims['prov_mean']
        features['provider_claim_std'] = claims['prov_std']
        
        # Deviation from provider's own average
        features['claim_vs_provider_avg'] = claims['claim_amount'] / (claims['prov_mean'] + 1)
        
        # 5. PATIENT FEATURES (NO LEAKAGE)
        features['patient_age'] = claims['age']
        features['patient_gender'] = (claims['gender'] == 'M').astype(int)
        
        # Patient's claim count
        patient_counts = claims.groupby('patient_id').size()
        features['patient_claim_count'] = claims['patient_id'].map(patient_counts)
        
        # How long they've been a member
        claims['member_since_dt'] = pd.to_datetime(claims['member_since'])
        claims['claim_date_dt'] = pd.to_datetime(claims['claim_date'])
        features['membership_days'] = (claims['claim_date_dt'] - claims['member_since_dt']).dt.days
        
        # 6. TEMPORAL FEATURES (NO LEAKAGE)
        features['day_of_week'] = claims['claim_date_dt'].dt.dayofweek
        features['day_of_month'] = claims['claim_date_dt'].dt.day
        features['month'] = claims['claim_date_dt'].dt.month
        features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)
        features['is_month_end'] = (features['day_of_month'] >= 28).astype(int)
        
        # Hour of day (if available)
        if 'claim_datetime' in claims.columns:
            claims['claim_datetime_dt'] = pd.to_datetime(claims['claim_datetime'])
            features['hour_of_day'] = claims['claim_datetime_dt'].dt.hour
            features['is_business_hours'] = ((features['hour_of_day'] >= 8) & 
                                            (features['hour_of_day'] <= 18)).astype(int)
            features['is_night'] = ((features['hour_of_day'] >= 22) | 
                                   (features['hour_of_day'] <= 6)).astype(int)
        else:
            features['hour_of_day'] = 12
            features['is_business_hours'] = 1
            features['is_night'] = 0
        
        # 7. MEDICAL CODE FEATURES (ENCODED - NO LEAKAGE)
        # Encode specialty
        specialty_encoder = LabelEncoder()
        features['specialty_encoded'] = specialty_encoder.fit_transform(claims['specialty_provider'])
        self.label_encoders['specialty'] = specialty_encoder
        
        # Encode CPT code
        cpt_encoder = LabelEncoder()
        features['cpt_encoded'] = cpt_encoder.fit_transform(claims['cpt_code'])
        self.label_encoders['cpt'] = cpt_encoder
        
        # Encode ICD code
        icd_encoder = LabelEncoder()
        features['icd_encoded'] = icd_encoder.fit_transform(claims['icd_code'])
        self.label_encoders['icd'] = icd_encoder
        
        # ICD chapter (first character)
        claims['icd_chapter'] = claims['icd_code'].str[0]
        icd_chapter_encoder = LabelEncoder()
        features['icd_chapter_encoded'] = icd_chapter_encoder.fit_transform(claims['icd_chapter'])
        self.label_encoders['icd_chapter'] = icd_chapter_encoder
        
        # 8. INTERACTION FEATURES (NO LEAKAGE)
        features['amount_x_age'] = features['claim_amount'] * features['patient_age']
        features['amount_x_experience'] = features['claim_amount'] * features['provider_years_experience']
        features['age_x_experience'] = features['patient_age'] * features['provider_years_experience']
        features['volume_x_amount'] = features['provider_claim_volume'] * features['log_amount']
        
        # Fill any NaN values
        features = features.fillna(0)
        
        print(f"   âœ… Created {len(features.columns)} features (NO fraud indicators used)")
        
        return features
    
    def prepare_data(self, claims_df: pd.DataFrame, providers_df: pd.DataFrame,
                    patients_df: pd.DataFrame, test_size: float = 0.65, 
                    val_size: float = 0.15) -> Tuple:
        """Prepare data with proper train/val/test split"""
        
        print("\nğŸ“Š Preparing data for ML training...")
        
        # Engineer features WITHOUT leakage
        X = self.engineer_features_no_leakage(claims_df, providers_df, patients_df)
        
        # Get target variable
        y = claims_df['is_fraudulent'].values
        
        # Store feature names
        self.feature_names = X.columns.tolist()
        
        # First split: separate test set
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # Second split: separate validation from training
        val_size_adjusted = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp
        )
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Convert back to DataFrame
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=self.feature_names, index=X_train.index)
        X_val_scaled = pd.DataFrame(X_val_scaled, columns=self.feature_names, index=X_val.index)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=self.feature_names, index=X_test.index)
        
        print(f"   âœ… Training set: {len(X_train)} samples ({y_train.mean()*100:.1f}% fraud)")
        print(f"   âœ… Validation set: {len(X_val)} samples ({y_val.mean()*100:.1f}% fraud)")
        print(f"   âœ… Test set: {len(X_test)} samples ({y_test.mean()*100:.1f}% fraud)")
        
        return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test
    
    def build_realistic_model(self):
        """Build model with proper regularization to avoid overfitting"""
        
        print("\nğŸ¤– Building ML model with regularization...")
        
        if XGBOOST_AVAILABLE and LIGHTGBM_AVAILABLE:
            # Use XGBoost with proper regularization
            model = XGBClassifier(
                n_estimators=150,
                max_depth=6,  # Reduced from 8 to prevent overfitting
                learning_rate=0.05,  # Lower learning rate
                subsample=0.7,  # Sample 70% of data for each tree
                colsample_bytree=0.7,  # Use 70% of features
                min_child_weight=3,  # Minimum samples in leaf
                gamma=0.1,  # Minimum loss reduction for split
                reg_alpha=0.1,  # L1 regularization
                reg_lambda=1.0,  # L2 regularization
                scale_pos_weight=7,  # Handle imbalance (873/127)
                random_state=42,
                n_jobs=-1,
                eval_metric='logloss',
                early_stopping_rounds=10
            )
            print("   âœ… Using XGBoost with regularization")
        else:
            # Fallback to Random Forest with regularization
            model = RandomForestClassifier(
                n_estimators=150,
                max_depth=10,  # Limit depth
                min_samples_split=20,  # Require more samples to split
                min_samples_leaf=10,  # Require more samples in leaf
                max_features='sqrt',  # Limit features per tree
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            )
            print("   âœ… Using Random Forest with regularization")
        
        return model
    
    def train(self, claims_df: pd.DataFrame, providers_df: pd.DataFrame,
             patients_df: pd.DataFrame) -> Dict:
        """Train the ML model with proper validation"""
        
        print("="*60)
        print("ğŸš€ TRAINING ML FRAUD DETECTION MODEL (NO DATA LEAKAGE)")
        print("="*60)
        
        # Prepare data with train/val/test split
        X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_data(
            claims_df, providers_df, patients_df
        )
        
        # Build model
        self.model = self.build_realistic_model()
        
        # Train model with validation monitoring
        print("\nâ³ Training model (expecting 30-60 seconds for proper training)...")
        
        if isinstance(self.model, XGBClassifier):
            # Train with early stopping on validation set
            self.model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                verbose=False
            )
        else:
            # Regular training
            self.model.fit(X_train, y_train)
        
        print("   âœ… Model trained successfully!")
        
        # Evaluate on validation set
        print("\nğŸ“Š Evaluating on validation set...")
        y_val_pred = self.model.predict(X_val)
        y_val_proba = self.model.predict_proba(X_val)[:, 1]
        
        val_metrics = {
            'accuracy': accuracy_score(y_val, y_val_pred),
            'precision': precision_score(y_val, y_val_pred, zero_division=0),
            'recall': recall_score(y_val, y_val_pred, zero_division=0),
            'f1_score': f1_score(y_val, y_val_pred, zero_division=0),
            'roc_auc': roc_auc_score(y_val, y_val_proba)
        }
        
        print("\n" + "="*60)
        print("ğŸ“ˆ VALIDATION SET PERFORMANCE")
        print("="*60)
        for metric, value in val_metrics.items():
            print(f"{metric.upper():20s}: {value:.4f} ({value*100:.2f}%)")
        
        # Evaluate on test set (final performance)
        print("\nğŸ“Š Evaluating on test set (held-out data)...")
        y_test_pred = self.model.predict(X_test)
        y_test_proba = self.model.predict_proba(X_test)[:, 1]
        
        test_metrics = {
            'accuracy': accuracy_score(y_test, y_test_pred),
            'precision': precision_score(y_test, y_test_pred, zero_division=0),
            'recall': recall_score(y_test, y_test_pred, zero_division=0),
            'f1_score': f1_score(y_test, y_test_pred, zero_division=0),
            'roc_auc': roc_auc_score(y_test, y_test_proba)
        }
        
        print("\n" + "="*60)
        print("ğŸ“ˆ TEST SET PERFORMANCE (FINAL)")
        print("="*60)
        for metric, value in test_metrics.items():
            print(f"{metric.upper():20s}: {value:.4f} ({value*100:.2f}%)")
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_test_pred)
        print("\nConfusion Matrix (Test Set):")
        print(cm)
        
        # Check for overfitting
        train_score = self.model.score(X_train, y_train)
        val_score = val_metrics['accuracy']
        test_score = test_metrics['accuracy']
        
        print("\nğŸ” Overfitting Check:")
        print(f"   Training accuracy:   {train_score:.4f}")
        print(f"   Validation accuracy: {val_score:.4f}")
        print(f"   Test accuracy:       {test_score:.4f}")
        
        if train_score - test_score > 0.10:
            print("   âš ï¸  Warning: Possible overfitting detected (>10% gap)")
        elif train_score - test_score > 0.05:
            print("   âš ï¸  Minor overfitting detected (5-10% gap)")
        else:
            print("   âœ… No significant overfitting detected!")
        
        # Feature importance
        self._calculate_feature_importance()
        
        # Save model
        self.save_model()
        
        return test_metrics
    
    def _calculate_feature_importance(self):
        """Calculate feature importance"""
        
        print("\nğŸ” Calculating feature importance...")
        
        if hasattr(self.model, 'feature_importances_'):
            self.feature_importance = pd.DataFrame({
                'feature': self.feature_names,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print("\nğŸ“Š Top 10 Most Important Features:")
            for idx, row in self.feature_importance.head(10).iterrows():
                print(f"   {row['feature']:30s}: {row['importance']:.4f}")
    
    def predict(self, claims_df: pd.DataFrame, providers_df: pd.DataFrame,
               patients_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Make predictions on new data"""
        
        if self.model is None:
            raise ValueError("Model not trained! Call train() first or load_model()")
        
        # Engineer features
        X = self.engineer_features_no_leakage(claims_df, providers_df, patients_df)
        
        # Scale
        X_scaled = self.scaler.transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=self.feature_names)
        
        # Predict
        predictions = self.model.predict(X_scaled)
        probabilities = self.model.predict_proba(X_scaled)[:, 1]
        
        return predictions, probabilities
    
    def save_model(self, filename: str = None):
        """Save trained model"""
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'fraud_detection_model_fixed_{timestamp}.pkl'
        
        filepath = os.path.join(self.model_dir, filename)
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'feature_importance': self.feature_importance,
            'label_encoders': self.label_encoders
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"\nğŸ’¾ Model saved to: {filepath}")
        
        # Also save as default
        default_path = os.path.join(self.model_dir, 'fraud_detection_model.pkl')
        with open(default_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ Model saved to: {default_path}")
    
    def load_model(self, filename: str = 'fraud_detection_model.pkl'):
        """Load trained model"""
        
        filepath = os.path.join(self.model_dir, filename)
        
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Model file not found: {filepath}")
        
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_names = model_data['feature_names']
        self.feature_importance = model_data.get('feature_importance')
        self.label_encoders = model_data.get('label_encoders', {})
        
        print(f"âœ… Model loaded from: {filepath}")


def main():
    """Main execution - Train FIXED ML model"""
    
    print("="*60)
    print("ğŸ¤– ML FRAUD DETECTION - FIXED VERSION (NO DATA LEAKAGE)")
    print("="*60 + "\n")
    
    # Load data
    print("ğŸ“‚ Loading data...")
    try:
        claims_df = pd.read_csv('data/raw/claims.csv')
        providers_df = pd.read_csv('data/raw/providers.csv')
        patients_df = pd.read_csv('data/raw/patients.csv')
        
        print(f"   âœ… Loaded {len(claims_df)} claims")
        print(f"   âœ… Loaded {len(providers_df)} providers")
        print(f"   âœ… Loaded {len(patients_df)} patients")
    except FileNotFoundError as e:
        print(f"   âŒ Error: {e}")
        print("   Please run: python data_generator.py")
        return
    
    # Initialize and train model
    ml_model = FraudDetectionMLModel()
    metrics = ml_model.train(claims_df, providers_df, patients_df)
    
    print("\n" + "="*60)
    print("âœ¨ TRAINING COMPLETE!")
    print("="*60)
    print("\nğŸ¯ Expected Performance: 85-95% accuracy (realistic)")
    print(f"ğŸ¯ Achieved Accuracy: {metrics['accuracy']*100:.2f}%")
    
    if metrics['accuracy'] > 0.98:
        print("\nâš ï¸  Warning: Very high accuracy may indicate remaining data leakage")
        print("   Please review features carefully")
    elif metrics['accuracy'] >= 0.85:
        print("\nâœ… Excellent performance! Realistic and production-ready.")
    else:
        print("\nâš ï¸  Lower than expected. May need more data or feature engineering.")
    
    print("\nğŸ¯ Next steps:")
    print("   1. Run: python etl_pipeline_ml.py")
    print("   2. Run: python model_metrics.py")
    print("   3. Run: streamlit run app.py")
    print("\n" + "="*60 + "\n")


if __name__ == "__main__":
    main()

